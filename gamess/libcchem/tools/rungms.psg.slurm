#!/bin/csh
set echo
#
#
#  Interactive: srun --pty -p hsw_p100 -n 4 -c 32 -t 24:0:0 /bin/bash -i
#  - requests 4 nodes
#  - sets environmental variable SLURM_CPUS_PER_TASK to 32
#    also sets OMP_NUM_THREADS to this value)
#
#  corresponding run should be:
#    ./rungms.psg.slurm input 00 4 1
#
# Sarom: Clean up any pre-existing semaphores
#
if ( -f "$HOME/bin/my_ipcrm" ) then
  $HOME/bin/my_ipcrm
endif
#
# Sarom: Grab the path to the current directory
#
set currentdir=`pwd`
#
if (-e $currentdir/install.info) then
   source $currentdir/install.info
else
   echo "Please run 'config' first, to set up GAMESS compiling information"
   exit 4
endif
if (! $?LD_LIBRARY_PATH ) then
  setenv LD_LIBRARY_PATH
endif
if ( $?GMS_CUDA_PATH ) then
  setenv LD_LIBRARY_PATH $GMS_CUDA_PATH/lib64:$GMS_CUDA_PATH/lib:$LD_LIBRARY_PATH
endif
#
# Sarom: set TARGET=ga for LIBCCHEM
# Sarom: set TARGET=mpi for GAMESS Fortran
#
set TARGET=ga
#
# Sarom: Must define path to a SCR directory.
if ( -d /scratch ) then
  set SCR=/scratch
else
  if ( -d /local ) then
    if ( -d /local/$SLURM_JOB_USER ) then
      set SCR=/local/$SLURM_JOB_USER
    else
      mkdir /local/$SLURM_JOB_USER
      set SCR=/local/$SLURM_JOB_USER
    endif
  endif
endif
#
# Sarom: Must define path to a USERSCR directory.
# Sarom: All files associated with restarts will be stored here.
#
set USERSCR=~/restart
#
# Sarom: Make life simple and set GMSPATH as the current directory
#
set GMSPATH=$currentdir
#
set JOB=$1      # name of the input file xxx.inp, give only the xxx part
set VERNO=$2    # revision number of the executable created by 'lked' step
set NCPUS=$3    # number of compute processes to be run
set NNODES=$SLURM_NNODES     # we are running on a single node
#
# provide defaults if last two arguments are not given to this script
if (null$VERNO == null) set VERNO=00
if (null$NCPUS == null) set NCPUS=1
#
#  ---- the top third of the script is input and other file assignments ----
#
echo "----- GAMESS execution script 'rungms' -----"
set master=`hostname`
echo This job is running on host $master
echo under operating system `uname` at `date`
echo "Available scratch disk space (Kbyte units) at beginning of the job is"
df -k $SCR
echo "GAMESS temporary binary files will be written to $SCR"
echo "GAMESS supplementary output files will be written to $USERSCR"
#
#        this added as experiment, February 2007, as 8 MBytes
#        increased to 32 MB in October 2013 for the VB2000 code.
#        its intent is to detect large arrays allocated off the stack
limit stacksize 32768
#
#  Grab a copy of the input file.
#  In the case of examNN jobs, file is in tests/standard subdirectory.
#  In the case of exam-vbNN jobs, file is in vb2000's tests subdirectory.
if ($JOB:r.inp == $JOB) then
  set JOB=$JOB:r      # strip off possible .inp
# Sarom: All previous $JOB related files in the scratch directory is deleted.
  rm -fv $SCR/$JOB*
endif
#
echo "Copying input file $JOB.inp to your run's scratch directory..."
if (-e $JOB.inp) then
   set echo
   cp  $JOB.inp  $SCR/$JOB.F05
   unset echo
else
   if (-e tests/standard/$JOB.inp) then
      set echo
      cp  tests/standard/$JOB.inp  $SCR/$JOB.F05
      unset echo
   else
      if (-e tests/$JOB.inp) then
         set echo
         cp  tests/$JOB.inp  $SCR/$JOB.F05
         unset echo
      else
         echo "Input file $JOB.inp does not exist."
         echo "This job expected the input file to be in directory `pwd`"
         echo "Please fix your file name problem, and resubmit."
         exit 4
      endif
   endif
endif
#
#    define many environment variables setting up file names.
#    anything can be overridden by a user's own choice, read 2nd.
#
source $GMSPATH/gms-files.csh
if (-e $HOME/.gmsrc) then
   echo "reading your own $HOME/.gmsrc"
   source $HOME/.gmsrc
endif
#
#        choose remote shell execution program.
#    Parallel run do initial launch of GAMESS on remote nodes by the
#    following program.  Note that the authentication keys for ssh
#    must have been set up correctly.
#    If you wish, choose 'rsh/rcp' using .rhosts authentication instead.
setenv DDI_RSH ssh
setenv DDI_RCP scp
#
#    If a $GDDI input group is present, the calculation will be using
#    subgroups within DDI (the input NGROUP=0 means this isn't GDDI).
#
#    The master within each group must have a copy of INPUT, which is
#    dealt with below (prior to execution), once we know something about
#    the host names where INPUT is required.  The INPUT does not have
#    the global rank appended to its name, unlike all other files.
#
#    OUTPUT and PUNCH (and perhaps many other files) are opened on all
#    processes (not just the master in each subgroup), but unique names
#    will be generated by appending the global ranks.  Note that OUTPUT
#    is not opened by the master in the first group, but is used by all
#    other groups.  Typically, the OUTPUT from the first group's master
#    is the only one worth saving, unless perhaps if runs crash out.
#
#    The other files that GDDI runs might use are already defined above.
#
set ngddi=`grep -i '^ \$GDDI' $SCR/$JOB.F05 | grep -iv 'NGROUP=0 ' | wc -l`
if ($ngddi > 0) then
   set GDDIjob=true
   echo "This is a GDDI run, keeping various output files on local disks"
   set echo
   setenv  OUTPUT $SCR/$JOB.F06
   setenv   PUNCH $SCR/$JOB.F07
   unset echo
else
   set GDDIjob=false
endif
#
#             replica-exchange molecular dynamics (REMD)
#     option is active iff runtyp=md as well as mremd=1 or 2.
#     It utilizes multiple replicas, one per subgroup.
#     Although REMD is indeed a GDDI kind of run, it handles its own
#     input file manipulations, but should do the GDDI file defs above.
set runmd=`grep -i runtyp=md $SCR/$JOB.F05 | wc -l`
set mremd=`grep -i mremd= $SCR/$JOB.F05 | grep -iv 'mremd=0 ' | wc -l`
if (($mremd > 0) && ($runmd > 0) && ($ngddi > 0)) then
   set GDDIjob=false
   set REMDjob=true
   echo "This is a REMD run, keeping various output files on local disks"
   set echo
   setenv TRAJECT     $SCR/$JOB.F04
   setenv RESTART $USERSCR/$JOB.rst
   setenv    REMD $USERSCR/$JOB.remd
   unset echo
   set GDDIinp=(`grep -i '^ \$GDDI' $JOB.inp`)
   set numkwd=$#GDDIinp
   @ g = 2
   @ gmax = $numkwd - 1
   while ($g <= $gmax)
      set keypair=$GDDIinp[$g]
      set keyword=`echo $keypair | awk '{split($1,a,"="); print a[1]}'`
      if (($keyword == ngroup) || ($keyword == NGROUP)) then
         set nREMDreplica=`echo $keypair | awk '{split($1,a,"="); print a[2]}'`
         @ g = $gmax
      endif
      @ g++
   end
   unset g
   unset gmax
   unset keypair
   unset keyword
else
   set REMDjob=false
endif
#
#    data left over from a previous run might be precious, stop if found.
if ((-e $PUNCH) || (-e $MAKEFP) || (-e $TRAJECT) || (-e $RESTART)) then
  set echo
    rm -rf $PUNCH
    rm -rf $MAKEFP
    rm -rf $TRAJECT
    rm -rf $RESTART
  unset echo
endif
#
#  ---- the middle third of the script is to execute GAMESS ----
#
#                     - a typical MPI example -
#
#         This section is customized to two possible MPI libraries:
#             Intel MPI
#
#         See ~/gamess/tools/gms, which is a front-end script to submit
#         this file 'rungms' as a back-end script, to either scheduler.
#
#                   if you are using some other MPI:
#         See ~/gamess/ddi/readme.ddi for information about launching
#         processes using other MPI libraries (each may be different).
#         Again: we do not know how to run openMPI effectively.
#
#                   if you are using some other batch scheduler:
#         Illustrating other batch scheduler's way's of providing the
#         hostname list is considered beyond the scope of this script.
#         Suffice it to say that
#             a) you will be given hostnames at run time
#             b) a typical way is a disk file, named by an environment
#                variable, containing the names in some format.
#             c) another typical way is an blank separated list in some
#                environment variable.
#         Either way, whatever the batch scheduler gives you must be
#         sliced-and-diced into the format required by your MPI kickoff.
#
if ($TARGET == mpi) then
   #
   #      Besides the usual three arguments to 'rungms' (see top),
   #      we'll pass in a "processers per node" value, that is,
   #      all nodes are presumed to have equal numbers of cores.
   #
   set PPN=$4
   if (null$PPN == null) set PPN=1
   #
   #      Allow for compute process and data servers (one pair per core)
   #      note that NCPUS = #cores, and NPROCS = #MPI processes
   #
   @ NPROCS = $NCPUS + $NCPUS
   #
   #      User customization required here:
   #       1. specify your MPI choice: impi
   #       2. specify your MPI library's top level path just below,
   #          this will have directories like include/lib/bin below it.
   #       3. a bit lower, perhaps specify your ifort path information.
   #
   set DDI_MPI_CHOICE=$GMS_MPI_LIB
   #
   if ($DDI_MPI_CHOICE == impi) then
      set DDI_MPI_ROOT=$GMS_MPI_PATH/intel64
   else
      set DDI_MPI_ROOT=$GMS_MPI_PATH
   endif
   #
   #        pre-pend our MPI choice to the library and execution paths.
   switch ($DDI_MPI_CHOICE)
      case impi:
      case mpich:
         setenv LD_LIBRARY_PATH $DDI_MPI_ROOT/lib:$LD_LIBRARY_PATH
         set path=($DDI_MPI_ROOT/bin $path)
         rehash
         breaksw
      default:
         breaksw
   endsw
   #
   #       you probably don't need to modify the kickoff style (see below).
   #
   if ($DDI_MPI_CHOICE == impi)     set MPI_KICKOFF_STYLE=hydra
   if ($DDI_MPI_CHOICE == mpich)    set MPI_KICKOFF_STYLE=hydra
   if ($DDI_MPI_CHOICE == openmpi)  set MPI_KICKOFF_STYLE=orte
   #
   #  Argonne's MPICH2, offers two possible kick-off procedures,
   #  guided by two disk files (A and B below).
   #  Other MPI implementations are often derived from Argonne's,
   #  and so usually offer these same two styles.
   #  For example, iMPI and MVAPICH2 can choose either "3steps" or "hydra",
   #  but openMPI uses its own Open Run Time Environment, "orte".
   #
   #  Kickoff procedure #1 uses mpd demons, which potentially collide
   #  if the same user runs multiple jobs that end up on the same nodes.
   #  This is called "3steps" here because three commands (mpdboot,
   #  mpiexec, mpdallexit) are needed to run.
   #
   #  Kickoff procedure #2 is little faster, easier to use, and involves
   #  only one command (mpiexec.hydra).  It is called "hydra" here.
   #
   #  A. build HOSTFILE,
   #     This file is explicitly used only by "3steps" initiation,
   #     but it is always used below during file cleaning,
   #     and while creating the PROCFILE at step B,
   #     so we always make it.
   #
   setenv HOSTFILE $SCR/$JOB.nodes.mpd
   if (-e $HOSTFILE) rm $HOSTFILE
   touch $HOSTFILE
   #
   srun hostname -s | sort -u >> $HOSTFILE
   set NNODES=$SLURM_NNODES
   #           uncomment next lines if you need to debug host configuration.
   echo '-----debug----'
   echo HOSTFILE $HOSTFILE contains
   cat $HOSTFILE
   echo '--------------'
   #
   #  B. the next file forces explicit "which process on what node" rules.
   #     The contents depend on the kickoff style.  This file is how
   #     we tell MPI to double-book the cores with two processes,
   #     thus accounting for both compute processes and data servers.
   #
   setenv PROCFILE $SCR/$JOB.processes.mpd
   if (-e $PROCFILE) rm $PROCFILE
   touch $PROCFILE

   switch ($MPI_KICKOFF_STYLE)

  case hydra:

   if ($NNODES == 1) then
             # when all processes are inside a single node, it is simple!
             # all MPI processes, whether compute processes or data servers,
             # are just in this node.   (note: NPROCS = 2*NCPUS!)
      @ PPN2 = $PPN + $PPN
      echo "`hostname`:$NPROCS" > $PROCFILE
   else
             # For more than one node, we want PPN compute processes on
             # each node, and of course, PPN data servers on each.
             # Hence, PPN2 is doubled up.
             # Front end script 'gms' is responsible to ensure that NCPUS
             # is a multiple of PPN, and that PPN is less than or equals
             # the actual number of cores in the node.
      @ PPN2 = $PPN + $PPN
      @ n=1
      while ($n <= $NNODES)
         set host=`sed -n -e "$n p" $HOSTFILE`
         set host=$host[1]
         echo "${host}:$PPN2" >> $PROCFILE
         @ n++
      end
   endif
   breaksw

   endsw
   #           uncomment next lines if you need to debug host configuration.
   echo '-----debug----'
   echo PROCFILE $PROCFILE contains
   cat $PROCFILE
   echo '--------------'
   #
   #     ==== values that influence the MPI operation ====
   #
   #     tunings below are specific to Intel MPI 3.2 and/or 4.0:
   #        a very important option avoids polling for incoming messages
   #           which allows us to compile DDI in pure "mpi" mode,
   #           and get sleeping data servers if the run is SCF level.
   #        trial and error showed process pinning slows down GAMESS runs,
   #        set debug option to 5 to see messages while kicking off,
   #        set debug option to 200 to see even more messages than that,
   #        set statistics option to 1 or 2 to collect messaging info,
   #        iMPI 4.0 on up defaults fabric to shm,dapl: dapl only is faster.
   #
   if ($DDI_MPI_CHOICE == impi) then
      set echo
      setenv I_MPI_WAIT_MODE enable
      setenv I_MPI_PIN disable
      setenv I_MPI_DEBUG 0
      setenv I_MPI_STATS 0
      # Force use of "shared memory copy" large message transfer mechanism
      # The "direct" mechanism was introduced and made default for IPS 2017,
      # and makes GAMESS hang when DD_GSum() is called. See IPS 2017 release notes
      # for more details.
      setenv I_MPI_SHM_LMT shm
      #              next two select highest speed mode of an Infiniband
      setenv I_MPI_FABRICS dapl
      setenv I_MPI_DAT_LIBRARY libdat2.so
      #              next two select TCP/IP, a slower way to use Infiniband.
      #              The device could be eth0 if IP over IB is not enabled.
      #--setenv I_MPI_FABRICS tcp
      #--setenv I_MPI_TCP_NETMASK ib0
      #      in case someone wants to try the "tag matching interface",
      #      an option which unfortunately ignores the WAIT_MODE in 4.0.2!
      #--setenv I_MPI_FABRICS tmi
      #--setenv I_MPI_TMI_LIBRARY libtmi.so
      #--setenv I_MPI_TMI_PROVIDER psm
      #--setenv TMI_CONFIG $DDI_MPI_ROOT/etc/tmi.conf
      unset echo
   endif
   #
   if ($DDI_MPI_CHOICE == mvapich2) then
      set echo
      setenv MV2_USE_BLOCKING 1
      setenv MV2_ENABLE_AFFINITY 0
      unset echo
   endif
   #
   if ($DDI_MPI_CHOICE == openmpi) then
      set echo
      setenv OMPI_MCA_mpi_yield_when_idle 1
      unset echo
   endif
   #
   #
   #         ... thus ends setting up the process initiation,
   #             tunings, pathnames, library paths, for the MPI.
   #
   #
   #    Compiler library setup (ifort)
   #        just ignore this (or comment out) if you're using gfortran.
   #        ISU's various clusters have various compiler paths, in this order:
   #              dynamo/chemphys2011/exalted/bolt/CyEnce/thebunny/CJ
   #
   #
   #    Math library setup (MKL or Atlas):
   #
   #          set up Intel MKL (math kernel library):
   #          GAMESS links MKL statically, for single threaded execution,
   #          so if you use MKL, you can probably skip this part.
   #             below are ISU's dynamo/CyEnce clusters
   #
   #setenv MKL_NUM_THREADS 1
   #
   #
   #   =========== runtime path/library setup is now finished! ===========
   #     any issues with paths and libraries can be debugged just below:
   #
   echo '-----debug----'
   echo the execution path is
   echo $path
   echo " "
   echo the library path is
   echo $LD_LIBRARY_PATH
   echo " "
   echo The dynamically linked libraries for this binary are
   echo  $GMSPATH/gamess.$VERNO.x :
   echo
   ldd $GMSPATH/gamess.$VERNO.x
   echo '--------------'
   #
   #           the next two setups are GAMESS-related
   #
   #     Set up Fragment MO runs (or other runs exploiting subgroups).
   #     One way to be sure that the master node of each subgroup
   #     has its necessary copy of the input file is to stuff a
   #     copy of the input file onto every single node right here.
   if ($GDDIjob == true) then
      set nmax=`wc -l $HOSTFILE`
      set nmax=$nmax[1]
      set lasthost=$master
      echo GDDI has to copy your input to every node....
      @ n=2   # input has already been copied into the master node.
      while ($n <= $nmax)
         set host=`sed -n -e "$n p" $HOSTFILE`
         set host=$host[1]
         if ($host != $lasthost) then
            echo $DDI_RCP $SCR/$JOB.F05 ${host}:$SCR/$JOB.F05
                 $DDI_RCP $SCR/$JOB.F05 ${host}:$SCR/$JOB.F05
            set lasthost=$host
         endif
         @ n++
      end
      #      The default for the logical node size is all cores existing
      #      in the physical node (just skip setting the value).
      #      Some FMO runs may benefit by choosing smaller logical node
      #      sizes, if the physical nodes have many cores.
      #      Perhaps, trial and error might show most efficient run times
      #      of your particular problem occur using 4 cores per logical node?
      #---setenv DDI_LOGICAL_NODE_SIZE 4
   endif
   #
   if ($REMDjob == true) then
      source $GMSPATH/tools/remd.csh $TARGET $nREMDreplica
      if ($status > 0) exit $status
   endif
   #
   #  Now, at last, we can actually kick-off the MPI processes...
   #
   echo "MPI kickoff will run GAMESS on $NCPUS cores in $NNODES nodes."
   echo "The binary to be executed is $GMSPATH/gamess.$VERNO.x"
   echo "MPI will run $NCPUS compute processes and $NCPUS data servers,"
   echo "    placing $PPN of each process type onto each node."
   echo "The scratch disk space on each node is $SCR, with free space"
   df -k $SCR
   #
   chdir $SCR
   #
   switch ($MPI_KICKOFF_STYLE)

   case hydra:
      if ($DDI_MPI_CHOICE == impi) then
         set echo
         setenv I_MPI_HYDRA_ENV all
         setenv I_MPI_PERHOST $PPN2
         unset echo
      endif
      set echo
      mpiexec.hydra -f $PROCFILE -n $NPROCS \
            $GMSPATH/gamess.$VERNO.x < /dev/null
      unset echo
      breaksw
   case orte:
      set echo
      orterun -np $NPROCS --npernode $PPN2 \
              $GMSPATH/gamess.$VERNO.x < /dev/null
      unset echo
      breaksw
   case default:
      echo rungms: No valid DDI-over-MPI startup procedure was chosen.
      exit
   endsw
   #
   #    keep HOSTFILE, as it is passed to the file erasing step below
   rm -f $PROCFILE
   #
endif
#      ------ end of the MPI execution section -------
if ($TARGET == ga) then
   #
   #      This section is used if and only if you run GAMESS+LIBCCHEM,
   #      over Global Arrays (GA) which is running over MPI.
   #
   #      To save space, the more verbose notes in the MPI section are
   #      not all here.  See the MPI section for extra comments.
   #
   #      LIBCCHEM wants only one process per assigned node, hence the
   #      hardwiring of processes per node to just 1.  In effect, the input
   #      value NCPUS, and thus NPROCS, are node counts, not core counts.
   #      Parallelization inside the nodes is handled by LIBCCHEM threads.
   #      The lack of data servers is due to GA as the message passing agent.
   #
   set PPN=1
   @ NPROCS = $NCPUS
   if ($SLURM_CPUS_PER_TASK > 1) then
     setenv OMP_NUM_THREADS $SLURM_CPUS_PER_TASK
   endif
   #
   #      User customization here!
   #         select MPI from just two: impi,mvapich2
   #         select MPI top level directory pathname.
   #
   set GA_MPI_CHOICE=$GMS_MPI_LIB
   #
   #        ISU's various clusters have various iMPI paths
   #          the examples are our exalted/bolt clusters
   if ($GA_MPI_CHOICE == impi) then
      set GA_MPI_ROOT=$GMS_MPI_PATH/intel64
   else
      set GA_MPI_ROOT=$GMS_MPI_PATH
   endif
   #
   #        pre-pend our MPI choice to the library and execution paths.
   switch ($GA_MPI_CHOICE)
      case impi:
      case mpich:
         setenv LD_LIBRARY_PATH $GA_MPI_ROOT/lib:$LD_LIBRARY_PATH
         set path=($GA_MPI_ROOT/bin $path)
         rehash
         breaksw
      default:
         breaksw
   endsw
   #
   if ($GA_MPI_CHOICE == impi)     set MPI_KICKOFF_STYLE=hydra
   if ($GA_MPI_CHOICE == mpich)    set MPI_KICKOFF_STYLE=hydra
   #
   #   ===== set up MPI control files to execute 1 process per node =====
   #
   #  A. build HOSTFILE,
   #
   setenv HOSTFILE $SCR/$JOB.nodes.mpd
   if (-e $HOSTFILE) rm $HOSTFILE
   touch $HOSTFILE
   #
   srun hostname -s | sort -u >> $HOSTFILE
   set NNODES=$SLURM_NNODES
   #           uncomment next lines if you need to debug host configuration.
   echo '-----debug----'
   echo HOSTFILE $HOSTFILE contains
   cat $HOSTFILE
   echo '--------------'
   #
   #  B. the next file forces explicit "which process on what node" rules.
   #
   setenv PROCFILE $SCR/$JOB.processes.mpd
   if (-e $PROCFILE) rm $PROCFILE
   touch $PROCFILE
   #
   switch ($MPI_KICKOFF_STYLE)

   case hydra:

   if ($NNODES == 1) then
             # when all processes are inside a single node, it is simple!
      echo "`hostname`:$PPN" > $PROCFILE
   else
      @ n=1
      while ($n <= $NNODES)
         set host=`sed -n -e "$n p" $HOSTFILE`
         set host=$host[1]
         echo "${host}:$PPN" >> $PROCFILE
         @ n++
      end
   endif
   breaksw

   endsw
   #
   #           uncomment next lines if you need to debug host configuration.
   echo '-----debug----'
   echo PROCFILE $PROCFILE contains
   cat $PROCFILE
   echo '--------------'
   #
   #  c) next line finds ifort-related compiler libraries
   #     ignore this, or comment out if you're using gfortran.
   #            the examples are our exalted/bolt clusters
   #
   #  d) next line finds Intel MKL (math kernel library) libraries
   #  While pure-GAMESS steps run, we want serial execution here, note that
   #  at times LIBCCHEM manipulates some of its steps to use threaded MKL.
   #  Atlas is an acceptable substitute for MKL, if you linked to Atlas.
   #            the examples are our exalted/bolt clusters
   #
   #setenv MKL_NUM_THREADS 1
   #
   #      any issues with run-time libraries can be debugged just below
   echo '-----debug libcchem----'
   echo the execution path is
   echo $path
   echo the library path is
   echo $LD_LIBRARY_PATH
   echo The dynamically linked libraries for this binary are
   echo $GMSPATH/gamess.cchem.$VERNO.x :
   echo
   ldd $GMSPATH/gamess.cchem.$VERNO.x
   echo '--------------'
   #
   #     ==== values that influence the MPI operation ====
   #
   #     There is a known problem with GA on QLogics brand infiniband,
   #     for which the high speed IB mode "dapl" does not work correctly.
   #     In our experience, Mellanox brand infiniband works OK.
   #
   #         our exalted/bolt clusters have QLogics/Mellanox boards.
   #
   if ($GA_MPI_CHOICE == impi) then
      set echo
      setenv I_MPI_WAIT_MODE enable
      setenv I_MPI_PIN disable
      setenv I_MPI_DEBUG 0
      setenv I_MPI_STATS 0
      # Force use of "shared memory copy" large message transfer mechanism
      # The "direct" mechanism was introduced and made default for IPS 2017,
      # and makes GAMESS hang when DD_GSum() is called. See IPS 2017 release notes
      # for more details.
      setenv I_MPI_SHM_LMT shm
      #      Qlogics Infiniband must run in IPoIB mode due to using GA.
      #         recently, device ib0 stopped working, but eth1 is OK.
      setenv I_MPI_FABRICS tcp
      setenv I_MPI_TCP_NETMASK eth1
      #      Mellanox Infiniband can launch GA in a native IB mode
      #--env I_MPI_FABRICS dapl
      #--env I_MPI_DAT_LIBRARY libdat2.so
      unset echo
   endif
   #
   #          ===== Runtime control over LIBCCHEM =====
   #      set GMS_CCHEM to 1 to enable calls to LIBCCHEM.
   #      set CCHEM to control use of GPUs, or memory used.
   #                for example, setenv CCHEM 'devices=;memory=100m'
   #                disables the usage of GPUs,
   #                and limits memory/node to 100m (units m,g both OK)
   #      set OMP_NUM_THREADS to limit core usage to fewer than all cores.
   #
   setenv GMS_CCHEM '1'
   #
   #      Our 'gms' front end for PBS batch submission changes the value
   #      of NUMGPU to 2 or 4 depending on user's request, or else
   #      leaves NUMGPU at 0 if the user decides to ignore any GPUs.
   #
   #      Please set to your number of GPU's if you are not using
   #      the front end 'gms' to correctly change this value.
   #      The 0 otherwise leads to ignoring GPUs (OK if you have none).
   #
   #      Approximately 1 GByte of memory should be given per CPU thread.
   #      Our system is hex-core nodes, your memory setting might vary.
   @ NUMGPU=1
   #
   # Sarom: Controls how much memory to give libcchem
   #
   setenv CCHEM 'devices=0;memory=225g'
   setenv CUDA_VISIBLE_DEVICES 0
   #
   #  Now, at last, we can actually kick-off the MPI/GA processes...
   #
   echo "MPI kickoff will start GAMESS on $NCPUS cores in $NNODES nodes."
   echo "LIBCCHEM will generate threads on all other cores in each node."
   echo "LIBCCHEM will run threads on $NUMGPU GPUs per node."
   echo "LIBCCHEM's control setting for CCHEM is $CCHEM"
   echo "The binary to be executed is $GMSPATH/gamess.cchem.$VERNO.x"
   echo "The scratch disk space on each node is $SCR, with free space"
   df -k $SCR
   chdir $SCR
   #
   switch ($MPI_KICKOFF_STYLE)
     case hydra:
       if ($GA_MPI_CHOICE == impi) then
         set echo
         setenv I_MPI_HYDRA_ENV all
         setenv I_MPI_PERHOST $PPN
         unset echo
       endif
       if ($GA_MPI_CHOICE == mpich) then
         set echo
         setenv HYDRA_ENV all
         setenv HYDRA_DEBUG 0
         unset echo
       endif
       set echo
       which mpiexec
       mpiexec.hydra -f $PROCFILE -n $NPROCS \
             $GMSPATH/gamess.cchem.$VERNO.x < /dev/null
       unset echo
       breaksw

     case default:
       echo No valid GA/MPI startup procedure chosen.
       exit
       breaksw
   endsw
   #
   #    keep HOSTFILE, as it is passed to the file erasing step below
   rm -f $PROCFILE
   #
endif
#      ------ end of the GA execution section -------
#
#  ---- the bottom third of the script is to clean up all disk files ----
#  It is quite useful to display to users how big the disk files got to be.
#
echo ----- accounting info -----
#
#   in the case of GDDI runs, we save the first PUNCH file only.
#   If something goes wrong, the .F06.00x, .F07.00x, ... from the
#   other groups are potentially interesting to look at.
if ($GDDIjob == true) cp $SCR/$JOB.F07 $USERSCR/$JOB.dat
#
#   Clean up the master's scratch directory.
#
echo Files used on the master node $master were:
ls -lF $SCR/$JOB.*
rm -f  $SCR/$JOB.F*
rm -fv $SCR/*
#
#   Clean/Rescue any files created by the VB2000 plug-in
if (-e $SCR/$JOB.V84)        mv $SCR/$JOB.V84     $USERSCR
if (-e $SCR/$JOB.V80)        rm -f $SCR/$JOB.V*
if (-e $SCR/$JOB.TEMP02)     rm -f $SCR/$JOB.TEMP*
if (-e $SCR/$JOB.orb)        mv $SCR/$JOB.orb     $USERSCR
if (-e $SCR/$JOB.vec)        mv $SCR/$JOB.vec     $USERSCR
if (-e $SCR/$JOB.mol)        mv $SCR/$JOB.mol     $USERSCR
if (-e $SCR/$JOB.molf)       mv $SCR/$JOB.molf    $USERSCR
if (-e $SCR/$JOB.mkl)        mv $SCR/$JOB.mkl     $USERSCR
if (-e $SCR/$JOB.xyz)        mv $SCR/$JOB.xyz     $USERSCR
ls $SCR/${JOB}-*.cube > $SCR/${JOB}.lis
if (! -z $SCR/${JOB}.lis) mv $SCR/${JOB}*.cube $USERSCR
rm -f $SCR/${JOB}.lis
ls $SCR/${JOB}-*.grd > $SCR/${JOB}.lis
if (! -z $SCR/${JOB}.lis) mv $SCR/${JOB}*.grd $USERSCR
rm -f $SCR/${JOB}.lis
ls $SCR/${JOB}-*.csv > $SCR/${JOB}.lis
if (! -z $SCR/${JOB}.lis) mv $SCR/${JOB}*.csv $USERSCR
rm -f $SCR/${JOB}.lis
#
#   Clean up scratch directory of remote nodes.
#
#   This may not be necessary, e.g. on a T3E where all files are in the
#   same directory, and just got cleaned out by the previous 'rm'.  Many
#   batch queue managers provide cleaning out of scratch directories.
#   It still may be interesting to the user to see the sizes of files.
#
#   The 'lasthost' business prevents multiple cleanup tries on SMP nodes.
#
if ($TARGET == sockets) then
   set nmax=${#HOSTLIST}
   set lasthost=$HOSTLIST[1]
   @ n=2   # master already cleaned above
   while ($n <= $nmax)
      set host=$HOSTLIST[$n]
      set host=`echo $host | cut -f 1 -d :`   # drop anything behind a colon
      if ($host != $lasthost) then
         echo Files from $host are:
         $DDI_RSH $host -l $USER -n "ls -l $SCR/$JOB.*"
         $DDI_RSH $host -l $USER -n "rm -f $SCR/$JOB.F*"
         set lasthost=$host
      endif
      @ n++
   end
endif
#
#    This particular example is for the combination iMPI, w/SGE or PBS.
#    We have inherited a file of unique node names from above.
#    There is an option to rescue the output files from group DDI runs,
#    such as FMO, in case you need to see the other group's outputs.
if ( -f "$HOSTFILE" ) then
  if ($TARGET == mpi) then
     set nnodes=`wc -l $HOSTFILE`
     set nnodes=$nnodes[1]
   @ n=1
   set master=`hostname`
           # burn off the .local suffix in our cluster's hostname
   set master=$master:r
   while ($n <= $nnodes)
      set host=`sed -n -e "$n p" $HOSTFILE`
           # in case of openMPI, unwanted stuff may follow the hostname
      set host=$host[1]
      if ($host != $master) then
         echo Files used on node $host were:
         #---------FMO rescue------
         #--if ($GDDIjob == true) then
         #--   echo "========= OUTPUT from node $host is =============="
         #--   ssh $host -l $USER "cat $SCR/$JOB.F06*"
         #--endif
         #---------FMO rescue------
         ssh $host -l $USER "ls -l $SCR/$JOB.*"
         ssh $host -l $USER "rm -f $SCR/$JOB.*"
      endif
      @ n++
   end
#          clean off the last file on the master's scratch disk.
   rm -f $HOSTFILE
   #
   if ($?I_MPI_STATS) then
      if ($I_MPI_STATS > 0) mv $SCR/stats.txt ~/$JOB.$NCPUS.stats
   endif
  endif
endif
#
#  and this is the end
#
date
time
~/bin/my_ipcrm
exit
